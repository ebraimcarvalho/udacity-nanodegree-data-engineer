{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkify Project\n",
    "\n",
    "Este projeto foi proposto pelo Nanodegree Data Engineer da Udacity.\n",
    "\n",
    "### Introdução\n",
    "\n",
    "**Modelagem de Dados com Postgres e ETL pipeline**\n",
    "\n",
    "Uma startup chamada Sparkify quer analisar os dados que eles tem coletados das músicas e atividades dos usuários no seu novo aplicativo de streaming de música. O time de análises está interessando em entender quais músicas os usuários estão ouvindo. Atualmente, eles não tem uma forma fácil de consultar esses dados, que estão num diretório em formato JSON, um com os dados dos logs das atividades dos usuarios no aplicativo e outro com os metadados das musicas na biblioteca do aplicativo.\n",
    "\n",
    "\n",
    "Eles gostariam que um engenheiro de dados criasse um banco de dados Postgres com as tabelas desenvolvidas para otimizar as consultas das análises das músicas reproduzidas. Seu papel é criar um database schema e um ETL pipeline para estas análises.\n",
    "\n",
    "### Descrição do Projeto\n",
    "\n",
    "Neste projeto, iremos aplicar o que aprendemos em modelagem de dados com Postgres e construir um pipeline para extrair, transformar e carregar os dados no banco de dados usando Python. Para completar o projeto, precisaremos definir as tabelas fatos e dimensão, usando um star schema para facilitar as consultas do time de análise de dados, além de escrever um pipeline de dados para transferir os dados dos arquivos locais em formato JSON para estas tabelas criadas no Postgres, usando Python e SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste em extrair os dados de duas fontes:\n",
    "\n",
    "- song_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"num_songs\": 1, \n",
    "\"artist_id\": \"ARMJAGH1187FB546F3\", \n",
    "\"artist_latitude\": 35.14968, \n",
    "\"artist_longitude\": -90.04892, \n",
    "\"artist_location\": \"Memphis, TN\", \n",
    "\"artist_name\": \"The Box Tops\", \n",
    "\"song_id\": \"SOCIWDW12A8C13D406\", \n",
    "\"title\": \"Soul Deep\", \n",
    "\"duration\": 148.03546, \n",
    "\"year\": 1969\n",
    "}`\n",
    "\n",
    "- log_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"artist\":\"Des'ree\",\n",
    "\"auth\":\"Logged In\",\n",
    "\"firstName\":\"Kaylee\",\n",
    "\"gender\":\"F\",\n",
    "\"itemInSession\":1,\n",
    "\"lastName\":\"Summers\",\n",
    "\"length\":246.30812,\n",
    "\"level\":\"free\",\n",
    "\"location\":\"Phoenix-Mesa-Scottsdale, AZ\",\n",
    "\"method\":\"PUT\",\n",
    "\"page\":\"NextSong\",\n",
    "\"registration\":1540344794796.0,\n",
    "\"sessionId\":139,\n",
    "\"song\":\"You Gotta Be\",\n",
    "\"status\":200,\n",
    "\"ts\":1541106106796,\n",
    "\"userAgent\":\"\\\"Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/537.36 (KHTML, like Gecko) Chrome\\/35.0.1916.153 Safari\\/537.36\\\"\",\n",
    "\"userId\":\"8\"\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print da tabela dos logs de eventos dos usuários:\n",
    "\n",
    "![Screenshot of log_data](images/log-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo Schema e tabelas de relacionamentos\n",
    "\n",
    "Como o intuito é facilitar as consultas para a área de análise de dados executar e obter dados para futuras análises, vamos modelar seguindo o modelo Star Schema, um processamento OLAP (On-line Analytical Processing), que é desenhado para performar melhor e tem as seguintes características:\n",
    "\n",
    "- Aplicação: No nível estratégico, auxilia na análise empresarial e tomada de decisões;\n",
    "- Funcionalidade: Gera análises e relatórios gerenciais com leitura otimizada;\n",
    "- Estrutura de dados: Poucos detalhes, pois tem alto nível de sumarização;\n",
    "- Armazenamento dos dados: Utiliza-se da Data Warehouse para otimizar o desempenho da grande quantidade de dados;\n",
    "- Usuários: Destinados aos gestores e time analítico;\n",
    "- Frequência de utilização: Baixa, conforme programação da empresa;\n",
    "- Volatilidade: Dados não sofrem alterações, pois os usuários apenas realizarão sua leitura.\n",
    "\n",
    "O OLAP (On-line Analytical Processing) é voltado para a tomada de decisões, proporciona uma visão dos dados orientado à análise, além de uma navegação rápida e flexível. O OLAP recebe dados do OLTP (On-line Transactional Processing) para que possa realizar as análises. Essa carga de dados acontece conforme a necessidade da empresa. Sendo um sistema para tomada de decisões, não realiza transações (INSERT, UPDATE, DELETE) pois sua finalidade são consultas. Possui dados atuais e históricos e não há necessidade de backups regularmente, sendo que ele possui informações do OLTP. Caso algo aconteça com a base OLAP basta fazer uma carga novamente.\n",
    "\n",
    "\n",
    "### Tabelas\n",
    "\n",
    "Vamos agora começar a definir nossas tabelas fato e tabelas dimensão, lembrando que uma tabela fato é um evento, uma venda, uma transação ocorrida, um fato transacional que ocorreu no nosso sistema, no caso, quando um usuário acessa o aplicativo sparkify e clica numa música para ouvir, esses dados da musica selecionada, do usuário, localização, página em que ele estava e etc, fazem parte do evento \"tocar música\", esse é um bom candidato para nossa tabela fato. As tabelas fatos geralmente são de dados numéricos e não categóricos. Resumindo:\n",
    "\n",
    "A tabela fato é a principal tabela do Data Warehouse, ela vai conectar nas dimensões. Nessa tabela são armazenadas duas coisas: as métricas, que são os fatos propriamente ditos, e as foreign keys, chaves que servem para ligar os dados das dimensões com a fato. Ou seja, a tabela fato é composta pelas métricas, que são tudo aquilo que a empresa quer medir, junto com as foreign keys, chaves que ligam às dimensões que descrevem essas métricas. As métricas são utilizadas para medir, quantificar algo, são sempre números provenientes de transações da empresa. Tudo que a empresa quer mensurar é métrica, geralmente sendo o que o usuário quer medir.\n",
    "\n",
    "Mas o que é uma Foreign Key? É uma chave estrangeira que serve para relacionar os dados entre as tabelas fato e dimensão.\n",
    "\n",
    "Já as tabelas Dimensão, são as categorias de cada entidade envolvida na tabela fato, que podem ser usadas para trazer mais informações sobre os dados e facilitar as análises. Os dados do usuário que reproduziu uma música é um exemplo para uma tabela dimensão, o nome, sobrenome, gênero, nível na aplicação são os dados dessa dimensão. Os dados da música selecionadas já fazem parte de outra dimensão, como o nome da música, o ano de lançamento, tempo de duração e etc.\n",
    "\n",
    "A Dimensão possui característica descritiva dentro do DW. Ela qualifica as informações provenientes da tabela Fato. Através dela é possível analisar os dados sob múltiplas perspectivas.\n",
    "\n",
    "### Fact Table\n",
    "\n",
    "- songplays: registros nos dados log de eventos associados com músicas reproduzidas\n",
    "    - songplay_id, \n",
    "    - start_time, \n",
    "    - user_id, \n",
    "    - level, \n",
    "    - song_id, \n",
    "    - artist_id, \n",
    "    - session_id, \n",
    "    - location, \n",
    "    - user_agent\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "- users: usuários do aplicativo\n",
    "    - user_id, \n",
    "    - first_name, \n",
    "    - last_name, \n",
    "    - gender, \n",
    "    - level\n",
    "    \n",
    "    \n",
    "- songs: músicas no banco de dados\n",
    "    - song_id, \n",
    "    - title, \n",
    "    - artist_id, \n",
    "    - year, \n",
    "    - duration\n",
    "    \n",
    "    \n",
    "- artists: Artistas das músicas no banco de dados\n",
    "    - artist_id, \n",
    "    - name, \n",
    "    - location, \n",
    "    - latitude, \n",
    "    - longitude\n",
    "\n",
    "\n",
    "- time: Timestamps dos registros das músicas reproduzidas quebrados em unidades específicas para facilitar futuras análises\n",
    "    - start_time, \n",
    "    - hour, \n",
    "    - day, \n",
    "    - week, \n",
    "    - month, \n",
    "    - year, \n",
    "    - weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "\n",
    "Primeiro vamos escrever as queries para dropar e criar as tabelas e selecionar as músicas, depois vamos criar o banco de dados e as tabelas, vou explicando linha a linha para fácil entendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABELAS\n",
    "\n",
    "drop_table_songplays = \"DROP TABLE IF EXISTS songplays\"\n",
    "drop_table_users = \"DROP TABLE IF EXISTS users\"\n",
    "drop_table_songs = \"DROP TABLE IF EXISTS songs\"\n",
    "drop_table_artists = \"DROP TABLE IF EXISTS artists\"\n",
    "drop_table_time = \"DROP TABLE IF EXISTS time\"\n",
    "\n",
    "# CREATE TABELAS\n",
    "\n",
    "create_table_songplays = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songplays\n",
    "    (songplay_id int PRIMARY KEY, \n",
    "    start_time date, \n",
    "    user_id text NOT NULL,\n",
    "    level text, \n",
    "    song_id text,\n",
    "    artist_id text,\n",
    "    session_id int,\n",
    "    location text,\n",
    "    user_agent text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_users = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS users\n",
    "  (user_id text PRIMARY KEY,\n",
    "  first_name text NOT NULL,\n",
    "  last_name text NOT NULL,\n",
    "  gender text,\n",
    "  level text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_songs = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS songs\n",
    "  (song_id text PRIMARY KEY,\n",
    "  title text NOT NULL,\n",
    "  artist_id text NOT NULL,\n",
    "  year int,\n",
    "  duration float NOT NULL)\n",
    "\"\"\")\n",
    "\n",
    "create_table_time = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS time\n",
    "    (start_time date PRIMARY KEY,\n",
    "     hour int, \n",
    "     day int, \n",
    "     week int, \n",
    "     month int, \n",
    "     year int, \n",
    "     weekday text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_artists = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS artists\n",
    "  (artist_id text PRIMARY KEY,\n",
    "  name text NOT NULL,\n",
    "  location text,\n",
    "  latitude float,\n",
    "  longitude float)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT REGISTROS\n",
    "\n",
    "insert_song = (\"\"\"\n",
    "  INSERT INTO songs\n",
    "  (song_id, title, artist_id, year, duration)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (song_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_artist = (\"\"\"\n",
    "  INSERT INTO artists\n",
    "  (artist_id, name, location, latitude, longitude)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (artist_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_time = (\"\"\"\n",
    "  INSERT INTO time\n",
    "  (start_time, hour, day, week, month, year, weekday)\n",
    "  VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (start_time) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_user = (\"\"\"\n",
    "  INSERT INTO users\n",
    "  (user_id, first_name, last_name, gender, level)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (user_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_songplays = (\"\"\"\n",
    "  INSERT INTO songplays\n",
    "  (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "  VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (songplay_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "# SONG SELECT\n",
    "\n",
    "song_select = (\"\"\"\n",
    "  SELECT song_id, artists.artist_id, duration\n",
    "  FROM songs \n",
    "  JOIN artists \n",
    "  ON songs.artist_id = artists.artist_id\n",
    "  WHERE songs.title = %s\n",
    "  AND artists.name = %s\n",
    "  AND songs.duration = %s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_queries = [create_table_songplays, create_table_users, create_table_songs, create_table_time, create_table_artists]\n",
    "\n",
    "drop_table_queries = [drop_table_songplays, drop_table_users, drop_table_songs, drop_table_artists, drop_table_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos começar a criar o database e as tabelas\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def main():\n",
    "    \"\"\"Inicia conexão e cursor para criar database, drop e criar as tabelas\"\"\"\n",
    "    \n",
    "    cur, conn = create_database()\n",
    "    \n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \"\"\"Cria o banco de dados, mas antes faz o drop do banco de dados \n",
    "    se ele já existe, e em seguida, cria-o novamente se não existe\"\"\"\n",
    "    \n",
    "    conn = psycopg2.connect(\"host=localhost dbname=postgres user=ebraim password=ebraim\")\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"DROP DATABASE sparkifydb\")\n",
    "    cur.execute(\"CREATE DATABASE sparkifydb WITH ENCODING 'utf-8'\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    conn = psycopg2.connect(\"host=localhost dbname=sparkifydb user=ebraim password=ebraim\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    return cur, conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"Drops all tables created on the database\"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"Create all tables on the database\"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL PIPELINE\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def main_etl():\n",
    "    \"\"\" Function used to extract, transform all data from song_data and log_data and load it into a PostgreSQL DB\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(\"host=localhost dbname=sparkifydb user=ebraim password=ebraim\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    process_data(cur, conn, filepath=\"data/song_data\", func=process_song_data)\n",
    "    process_data(cur, conn, filepath=\"data/log_data\", func=process_log_data)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"Walks through all files nested under filepath, and processes all logs found.\n",
    "\n",
    "    Parameters:\n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      conn (psycopg2.connect()): Connection to the sparkifycdb database\n",
    "      filepath (str): Filepath parent of the logs to be analyzed\n",
    "      func (python function): Function to be used to process each log\n",
    "\n",
    "    Returns:\n",
    "      Name of files processed\n",
    "    \"\"\"\n",
    "\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            all_files.append(os.path.abspath(f))\n",
    "  \n",
    "    # iterate over files and process\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        func(cur, datafile)\n",
    "        conn.commit()\n",
    "\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def process_song_data (cur, datafile):\n",
    "    \"\"\" Reads songs log file row by row, selects needed fields and inserts them into song and artist tables.\n",
    "\n",
    "    Parameters:\n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      filepath (str): Filepath of the file to be analyzed\n",
    "    \"\"\"\n",
    "\n",
    "    # open song file\n",
    "    df = pd.read_json(datafile, lines=True)\n",
    "\n",
    "    for value in df.values:\n",
    "        num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year = value\n",
    "\n",
    "        # insert artist record\n",
    "        artist_data = [artist_id, artist_name, artist_location, artist_latitude, artist_longitude]\n",
    "        cur.execute(insert_artist, artist_data)\n",
    "\n",
    "        song_data = [song_id, title, artist_id, year, duration]\n",
    "        cur.execute(insert_song, song_data)\n",
    "\n",
    "\n",
    "def process_log_data (cur, datafile):\n",
    "    \"\"\" Reads user activity log file row by row, filters by NextSong, selects needed fields, transforms them and inserts them into time, user and songplay tables.\n",
    "\n",
    "    Parameters: \n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      filepath (str): Filepath of the file to be analyzed\n",
    "    \"\"\"\n",
    "\n",
    "    # open log file\n",
    "    df = pd.read_json(datafile, lines=True)\n",
    "\n",
    "    # filter by NextSong action\n",
    "    df = df[df['page'] == 'NextSong']\n",
    "\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.to_datetime(df['ts'], unit='ms')\n",
    "\n",
    "    # insert time records\n",
    "    time_data = []\n",
    "    for line in t:\n",
    "        time_data.append([line, line.hour, line.day, line.week, line.month, line.year, line.day_name()])\n",
    "    column_labels = ('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday')\n",
    "    time_df = pd.DataFrame(time_data, columns=column_labels)\n",
    "\n",
    "    for i, row in time_df.iterrows():\n",
    "        cur.execute(insert_time, list(row))\n",
    "\n",
    "    # load user table\n",
    "    user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "\n",
    "    # insert user records\n",
    "    for i, row in user_df.iterrows():\n",
    "        cur.execute(insert_user, list(row))\n",
    "\n",
    "    # insert songplay records\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        # get songid and artistid from song and artist tables\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid, duration = results\n",
    "        else:\n",
    "            songid, artistid, duration = None, None, None\n",
    "\n",
    "        # insert songplay record\n",
    "        # songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "        songplay_data = (i, pd.to_datetime(row.ts, unit='ms'), int(row.userId), row.level, songid, artistid, row.sessionId, row.location, row.userAgent)\n",
    "        cur.execute(insert_songplays, songplay_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
