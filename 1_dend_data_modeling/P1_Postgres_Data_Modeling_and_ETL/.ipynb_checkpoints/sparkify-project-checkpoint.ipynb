{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkify Project\n",
    "\n",
    "Este projeto foi proposto pelo Nanodegree Data Engineer da Udacity.\n",
    "\n",
    "### Introdução\n",
    "\n",
    "**Modelagem de Dados com Postgres e ETL pipeline**\n",
    "\n",
    "Uma startup chamada Sparkify quer analisar os dados que eles tem coletados das músicas e atividades dos usuários no seu novo aplicativo de streaming de música. O time de análises está interessando em entender quais músicas os usuários estão ouvindo. Atualmente, eles não tem uma forma fácil de consultar esses dados, que estão num diretório em formato JSON, um com os dados dos logs das atividades dos usuarios no aplicativo e outro com os metadados das musicas na biblioteca do aplicativo.\n",
    "\n",
    "\n",
    "Eles gostariam que um engenheiro de dados criasse um banco de dados Postgres com as tabelas desenvolvidas para otimizar as consultas das análises das músicas reproduzidas. Seu papel é criar um database schema e um ETL pipeline para estas análises.\n",
    "\n",
    "### Descrição do Projeto\n",
    "\n",
    "Neste projeto, iremos aplicar o que aprendemos em modelagem de dados com Postgres e construir um pipeline para extrair, transformar e carregar os dados no banco de dados usando Python. Para completar o projeto, precisaremos definir as tabelas fatos e dimensão, usando um star schema para facilitar as consultas do time de análise de dados, além de escrever um pipeline de dados para transferir os dados dos arquivos locais em formato JSON para estas tabelas criadas no Postgres, usando Python e SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste em extrair os dados de duas fontes:\n",
    "\n",
    "- song_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"num_songs\": 1, \n",
    "\"artist_id\": \"ARMJAGH1187FB546F3\", \n",
    "\"artist_latitude\": 35.14968, \n",
    "\"artist_longitude\": -90.04892, \n",
    "\"artist_location\": \"Memphis, TN\", \n",
    "\"artist_name\": \"The Box Tops\", \n",
    "\"song_id\": \"SOCIWDW12A8C13D406\", \n",
    "\"title\": \"Soul Deep\", \n",
    "\"duration\": 148.03546, \n",
    "\"year\": 1969\n",
    "}`\n",
    "\n",
    "- log_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"artist\":\"Des'ree\",\n",
    "\"auth\":\"Logged In\",\n",
    "\"firstName\":\"Kaylee\",\n",
    "\"gender\":\"F\",\n",
    "\"itemInSession\":1,\n",
    "\"lastName\":\"Summers\",\n",
    "\"length\":246.30812,\n",
    "\"level\":\"free\",\n",
    "\"location\":\"Phoenix-Mesa-Scottsdale, AZ\",\n",
    "\"method\":\"PUT\",\n",
    "\"page\":\"NextSong\",\n",
    "\"registration\":1540344794796.0,\n",
    "\"sessionId\":139,\n",
    "\"song\":\"You Gotta Be\",\n",
    "\"status\":200,\n",
    "\"ts\":1541106106796,\n",
    "\"userAgent\":\"\\\"Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/537.36 (KHTML, like Gecko) Chrome\\/35.0.1916.153 Safari\\/537.36\\\"\",\n",
    "\"userId\":\"8\"\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print da tabela dos logs de eventos dos usuários:\n",
    "\n",
    "![Screenshot of log_data](images/log-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo Schema e tabelas de relacionamentos\n",
    "\n",
    "Como o intuito é facilitar as consultas para a área de análise de dados executar e obter dados para futuras análises, vamos modelar seguindo o modelo Star Schema, um processamento OLAP (On-line Analytical Processing), que é desenhado para performar melhor e tem as seguintes características:\n",
    "\n",
    "- Aplicação: No nível estratégico, auxilia na análise empresarial e tomada de decisões;\n",
    "- Funcionalidade: Gera análises e relatórios gerenciais com leitura otimizada;\n",
    "- Estrutura de dados: Poucos detalhes, pois tem alto nível de sumarização;\n",
    "- Armazenamento dos dados: Utiliza-se da Data Warehouse para otimizar o desempenho da grande quantidade de dados;\n",
    "- Usuários: Destinados aos gestores e time analítico;\n",
    "- Frequência de utilização: Baixa, conforme programação da empresa;\n",
    "- Volatilidade: Dados não sofrem alterações, pois os usuários apenas realizarão sua leitura.\n",
    "\n",
    "O OLAP (On-line Analytical Processing) é voltado para a tomada de decisões, proporciona uma visão dos dados orientado à análise, além de uma navegação rápida e flexível. O OLAP recebe dados do OLTP (On-line Transactional Processing) para que possa realizar as análises. Essa carga de dados acontece conforme a necessidade da empresa. Sendo um sistema para tomada de decisões, não realiza transações (INSERT, UPDATE, DELETE) pois sua finalidade são consultas. Possui dados atuais e históricos e não há necessidade de backups regularmente, sendo que ele possui informações do OLTP. Caso algo aconteça com a base OLAP basta fazer uma carga novamente.\n",
    "\n",
    "\n",
    "### Tabelas\n",
    "\n",
    "Vamos agora começar a definir nossas tabelas fato e tabelas dimensão, lembrando que uma tabela fato é um evento, uma venda, uma transação ocorrida, um fato transacional que ocorreu no nosso sistema, no caso, quando um usuário acessa o aplicativo sparkify e clica numa música para ouvir, esses dados da musica selecionada, do usuário, localização, página em que ele estava e etc, fazem parte do evento \"tocar música\", esse é um bom candidato para nossa tabela fato. As tabelas fatos geralmente são de dados numéricos e não categóricos. Resumindo:\n",
    "\n",
    "A tabela fato é a principal tabela do Data Warehouse, ela vai conectar nas dimensões. Nessa tabela são armazenadas duas coisas: as métricas, que são os fatos propriamente ditos, e as foreign keys, chaves que servem para ligar os dados das dimensões com a fato. Ou seja, a tabela fato é composta pelas métricas, que são tudo aquilo que a empresa quer medir, junto com as foreign keys, chaves que ligam às dimensões que descrevem essas métricas. As métricas são utilizadas para medir, quantificar algo, são sempre números provenientes de transações da empresa. Tudo que a empresa quer mensurar é métrica, geralmente sendo o que o usuário quer medir.\n",
    "\n",
    "Mas o que é uma Foreign Key? É uma chave estrangeira que serve para relacionar os dados entre as tabelas fato e dimensão.\n",
    "\n",
    "Já as tabelas Dimensão, são as categorias de cada entidade envolvida na tabela fato, que podem ser usadas para trazer mais informações sobre os dados e facilitar as análises. Os dados do usuário que reproduziu uma música é um exemplo para uma tabela dimensão, o nome, sobrenome, gênero, nível na aplicação são os dados dessa dimensão. Os dados da música selecionadas já fazem parte de outra dimensão, como o nome da música, o ano de lançamento, tempo de duração e etc.\n",
    "\n",
    "A Dimensão possui característica descritiva dentro do DW. Ela qualifica as informações provenientes da tabela Fato. Através dela é possível analisar os dados sob múltiplas perspectivas.\n",
    "\n",
    "### Fact Table\n",
    "\n",
    "- songplays: registros nos dados log de eventos associados com músicas reproduzidas\n",
    "    - songplay_id, \n",
    "    - start_time, \n",
    "    - user_id, \n",
    "    - level, \n",
    "    - song_id, \n",
    "    - artist_id, \n",
    "    - session_id, \n",
    "    - location, \n",
    "    - user_agent\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "- users: usuários do aplicativo\n",
    "    - user_id, \n",
    "    - first_name, \n",
    "    - last_name, \n",
    "    - gender, \n",
    "    - level\n",
    "    \n",
    "    \n",
    "- songs: músicas no banco de dados\n",
    "    - song_id, \n",
    "    - title, \n",
    "    - artist_id, \n",
    "    - year, \n",
    "    - duration\n",
    "    \n",
    "    \n",
    "- artists: Artistas das músicas no banco de dados\n",
    "    - artist_id, \n",
    "    - name, \n",
    "    - location, \n",
    "    - latitude, \n",
    "    - longitude\n",
    "\n",
    "\n",
    "- time: Timestamps dos registros das músicas reproduzidas quebrados em unidades específicas para facilitar futuras análises\n",
    "    - start_time, \n",
    "    - hour, \n",
    "    - day, \n",
    "    - week, \n",
    "    - month, \n",
    "    - year, \n",
    "    - weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "\n",
    "Primeiro vamos escrever as queries para dropar e criar as tabelas e a query para selecionar as músicas que coincidem com o nome do artista e da música para inserir esses dados na tabela fato songplays, depois vamos criar o banco de dados e as tabelas, vou explicando linha a linha para fácil entendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPA AS TABELAS\n",
    "\n",
    "drop_table_songplays = \"DROP TABLE IF EXISTS songplays\"\n",
    "drop_table_users = \"DROP TABLE IF EXISTS users\"\n",
    "drop_table_songs = \"DROP TABLE IF EXISTS songs\"\n",
    "drop_table_artists = \"DROP TABLE IF EXISTS artists\"\n",
    "drop_table_time = \"DROP TABLE IF EXISTS time\"\n",
    "\n",
    "# CRIA TABELAS\n",
    "\n",
    "create_table_songplays = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songplays\n",
    "    (songplay_id int PRIMARY KEY, \n",
    "    start_time date, \n",
    "    user_id text NOT NULL,\n",
    "    level text, \n",
    "    song_id text,\n",
    "    artist_id text,\n",
    "    session_id int,\n",
    "    location text,\n",
    "    user_agent text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_users = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS users\n",
    "  (user_id text PRIMARY KEY,\n",
    "  first_name text NOT NULL,\n",
    "  last_name text NOT NULL,\n",
    "  gender text,\n",
    "  level text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_songs = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS songs\n",
    "  (song_id text PRIMARY KEY,\n",
    "  title text NOT NULL,\n",
    "  artist_id text NOT NULL,\n",
    "  year int,\n",
    "  duration float NOT NULL)\n",
    "\"\"\")\n",
    "\n",
    "create_table_time = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS time\n",
    "    (start_time date PRIMARY KEY,\n",
    "     hour int, \n",
    "     day int, \n",
    "     week int, \n",
    "     month int, \n",
    "     year int, \n",
    "     weekday text)\n",
    "\"\"\")\n",
    "\n",
    "create_table_artists = (\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS artists\n",
    "  (artist_id text PRIMARY KEY,\n",
    "  name text NOT NULL,\n",
    "  location text,\n",
    "  latitude float,\n",
    "  longitude float)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERE REGISTROS\n",
    "\n",
    "insert_song = (\"\"\"\n",
    "  INSERT INTO songs\n",
    "  (song_id, title, artist_id, year, duration)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (song_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_artist = (\"\"\"\n",
    "  INSERT INTO artists\n",
    "  (artist_id, name, location, latitude, longitude)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (artist_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_time = (\"\"\"\n",
    "  INSERT INTO time\n",
    "  (start_time, hour, day, week, month, year, weekday)\n",
    "  VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (start_time) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_user = (\"\"\"\n",
    "  INSERT INTO users\n",
    "  (user_id, first_name, last_name, gender, level)\n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (user_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "insert_songplays = (\"\"\"\n",
    "  INSERT INTO songplays\n",
    "  (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "  VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (songplay_id) DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "# SELECIONA REGISTROS QUE COINCIDAM COM O NOME DA MUSICA E DO ARTISTA\n",
    "\n",
    "song_select = (\"\"\"\n",
    "  SELECT song_id, artists.artist_id, duration\n",
    "  FROM songs \n",
    "  JOIN artists \n",
    "  ON songs.artist_id = artists.artist_id\n",
    "  WHERE songs.title = %s\n",
    "  AND artists.name = %s\n",
    "  AND songs.duration = %s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste momento temos as queries criadas, vamos disponibilizá-las num array para iterar posteriormente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_queries = [create_table_songplays, create_table_users, create_table_songs, create_table_time, create_table_artists]\n",
    "\n",
    "drop_table_queries = [drop_table_songplays, drop_table_users, drop_table_songs, drop_table_artists, drop_table_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos começar a criar o database e as tabelas\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def main():\n",
    "    \"\"\"Inicia conexão e cursor para criar database, dropar e criar as tabelas\"\"\"\n",
    "    \n",
    "    cur, conn = create_database()\n",
    "    \n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que a primeira função que será chamada é a função main, a partir dela serão chamadas outras funções.\n",
    "\n",
    "A segunda função chamada será a create_database, que retornará o cursor e a conexão do banco de dados que vamos utilizar nas outras funções para executar os comdandos das queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \"\"\"Cria o banco de dados, mas antes faz o drop do banco de dados \n",
    "    se ele já existe, e em seguida, cria-o novamente se não existe\"\"\"\n",
    "    \n",
    "    conn = psycopg2.connect(\"host=localhost dbname=postgres user=ebraim password=ebraim\")\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"DROP DATABASE sparkifydb\")\n",
    "    cur.execute(\"CREATE DATABASE sparkifydb WITH ENCODING 'utf-8'\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    conn = psycopg2.connect(\"host=localhost dbname=sparkifydb user=ebraim password=ebraim\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    return cur, conn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função create_database inicia fazendo uma conexão ao database postgres com usuario e senha para acesso.\n",
    "O comando `conn.set_session(autocommit=True)` serve para comitar de forma automática todas vez que executarmos algum comando. Por que precisamos comitar todas execuções? Bem vamos entrar no conceito de ACID.\n",
    "\n",
    "A sigla ACID significa Atomicity, Consistency, Isolation, Durability, ou em português Atomicidade, Consistência, Isolamento e Durabilidade, e diz respeito a um conjunto de propriedades em transações de bancos de dados que são importantes para garantir a validade dos dados mesmo que ocorram erros durante o armazenamento ou problemas mais graves no sistema, como crashes ou problemas físicos em um servidor. As propriedades ACID são fundamentais para o processamento de transações em bancos de dados. Uma transação é, basicamente, uma sequência de operações que satisfazem a essas quatro propriedades.\n",
    "\n",
    "#### Vamos nos atentar ao primeiro item, Atomicidade: \n",
    "\n",
    "As transações são, geralmente, compostas de várias declarações (comandos / operações). A atomicidade é uma propriedade que garante que cada transação seja tratada como uma entidade única, a qual deve ser executada por completo ou falhar completamente. Desta forma, todas as operações da transação devem ser executadas com sucesso para que a transação tenha sucesso.\n",
    "Se uma única operação que seja do bloco da transação falhar, toda a transação deverá ser cancelada – as transações são aplicadas de uma forma “tudo ou nada”. Caso haja falha em qualquer operação da transação, o banco de dados será retornado ao estado anterior ao início da transação. Chamamos a esse retorno de estado de Rollback (“transação desfeita”). Caso a transação tenha sucesso, o BD é alterado de forma permanente, em um processo denominado Commit (“efetivação”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"Drops all tables created on the database\"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"Create all tables on the database\"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções drop_tables e create_tables recebem por parâmetros, o cursor e a conexão criada anteriormente pela função create_database, em seguida elas percorrem o array/lista criado anteriormente (drop_table_queries e create_table_queries) e executa cada comando que criamos, para dropar as tabelas e em seguida para criá-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamamos a função main e temos nosso banco de dados e tabelas criadas, os últimos comandos da função fecham o cursor e a conexão.\n",
    "\n",
    "Agora podemos começar a trabalhar no Pipeline para Extrair, transformar e inserir os dados nessas tabelas criadas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL PIPELINE\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def main_etl():\n",
    "    \"\"\" Function used to extract, transform all data from song_data and log_data and load it into a PostgreSQL DB\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(\"host=localhost dbname=sparkifydb user=ebraim password=ebraim\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    process_data(cur, conn, filepath=\"data/song_data\", func=process_song_data)\n",
    "    process_data(cur, conn, filepath=\"data/log_data\", func=process_log_data)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"Walks through all files nested under filepath, and processes all logs found.\n",
    "\n",
    "    Parameters:\n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      conn (psycopg2.connect()): Connection to the sparkifycdb database\n",
    "      filepath (str): Filepath parent of the logs to be analyzed\n",
    "      func (python function): Function to be used to process each log\n",
    "\n",
    "    Returns:\n",
    "      Name of files processed\n",
    "    \"\"\"\n",
    "\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            all_files.append(os.path.abspath(f))\n",
    "  \n",
    "    # iterate over files and process\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        func(cur, datafile)\n",
    "        conn.commit()\n",
    "\n",
    "    return all_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Início do ETL pipeline\n",
    "\n",
    "Nossa Primeira função chamada será a main_etl, ela será responsável por fazer a conexão com o banco de dados, chamar a função process_data para os dados na pasta song_data (que servirão para inserir os dados nas tabelas songs e artists) e posteriormente, chamar a mesma função process_data para os dados da pasta log_data (que servirão para inserir os dados nas tabelas time, users e songplays).\n",
    "\n",
    "A função process_data recebe como parâmetros o cursor, a conexão, o caminho da pasta onde estão os arquivos para percorrer e a função específica para fazer o ETL para esses dois conjuntos de dados.\n",
    "\n",
    "A função process_data começa declarando um array/lista que receberá os caminhos dos arquivos para serem percorridos futuramente, usamos um for loop para percorrer primeiramente todos os arquivos da pasta raiz passada como parâmetro, em seguida, unimos o caminho relativo com o formato .json e dentro do primeiro loop criado usamos outro for loop para adicionar cada um ao final do nosso array com a função append.\n",
    "\n",
    "Por último, percorremos esse array `all_files` chamando a função passada como parâmetro e passamos para frente o cursor para executar os comandos nas tabelas e caminho de cada arquivo da pasta que será lido com auxílio da pandas para fazer esse meio de campo. Após cada iteração, comitamos, e no final a função retornará o array com os caminhos de todos os arquivos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data (cur, datafile):\n",
    "    \"\"\" Reads songs log file row by row, selects needed fields and inserts them into song and artist tables.\n",
    "\n",
    "    Parameters:\n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      filepath (str): Filepath of the file to be analyzed\n",
    "    \"\"\"\n",
    "\n",
    "    # open song file\n",
    "    df = pd.read_json(datafile, lines=True)\n",
    "\n",
    "    for value in df.values:\n",
    "        num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year = value\n",
    "\n",
    "        # insert artist record\n",
    "        artist_data = [artist_id, artist_name, artist_location, artist_latitude, artist_longitude]\n",
    "        cur.execute(insert_artist, artist_data)\n",
    "\n",
    "        song_data = [song_id, title, artist_id, year, duration]\n",
    "        cur.execute(insert_song, song_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processando os dados da pasta song_data\n",
    "\n",
    "Chegamos na parte de processar os dados, transformar e inserir nas nossas tabelas do banco de dados.\n",
    "Lembrando que a função será chamada para cada arquivo na pasta (a função esta sendo chamada dentro de um for loop percorrendo a lista com todos os arquivos!\n",
    "\n",
    "Recebemos como parâmetros da função, o cursor e o arquivo do dado (o caminho do arquivo em formato json)\n",
    "\n",
    "Nosso primeiro trabalho é ler esse arquivo com o auxílio do pandas, e para isso, usamos a função read_json para ler esse arquivo e atribuimos à variavel df.\n",
    "\n",
    "Em seguida, vamos usar um for loop para percorrer os valores desse dataframe, para cada linha do dataframe, extrairemos cada registro e alocaremos em uma variavel, essas variaveis serao usadas para serem inseridas nas tabelas.\n",
    "\n",
    "\n",
    "#### Inserindo os dados na tabela dimensão artists\n",
    "\n",
    "Seguindo a linha do raciocínio, definimos uma lista com os dados para a tabela artists, em seguida, executamos o cursor para inserir na tabela artists, a lista dos dados dos artistas criados agora pouco.\n",
    "\n",
    "#### Inserindo os dados na tabela dimensão songs\n",
    "Em seguida, separamos os dados das musicas numa outra lista e executamos o cursor novamente, dessa vez para inserir na tabela songs os dados dessa lista criada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data (cur, datafile):\n",
    "    \"\"\" Reads user activity log file row by row, filters by NextSong, selects needed fields, transforms them and inserts them into time, user and songplay tables.\n",
    "\n",
    "    Parameters: \n",
    "      cur (psycopg2.cursor()): Cursor of the sparkifydb database\n",
    "      filepath (str): Filepath of the file to be analyzed\n",
    "    \"\"\"\n",
    "\n",
    "    # open log file\n",
    "    df = pd.read_json(datafile, lines=True)\n",
    "\n",
    "    # filter by NextSong action\n",
    "    df = df[df['page'] == 'NextSong']\n",
    "\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.to_datetime(df['ts'], unit='ms')\n",
    "\n",
    "    # insert time records\n",
    "    time_data = []\n",
    "    for line in t:\n",
    "        time_data.append([line, line.hour, line.day, line.week, line.month, line.year, line.day_name()])\n",
    "    column_labels = ('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday')\n",
    "    time_df = pd.DataFrame(time_data, columns=column_labels)\n",
    "\n",
    "    for i, row in time_df.iterrows():\n",
    "        cur.execute(insert_time, list(row))\n",
    "\n",
    "    # load user table\n",
    "    user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "\n",
    "    # insert user records\n",
    "    for i, row in user_df.iterrows():\n",
    "        cur.execute(insert_user, list(row))\n",
    "\n",
    "    # insert songplay records\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        # get songid and artistid from song and artist tables\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid, duration = results\n",
    "        else:\n",
    "            songid, artistid, duration = None, None, None\n",
    "\n",
    "        # insert songplay record\n",
    "        # songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "        songplay_data = (i, pd.to_datetime(row.ts, unit='ms'), int(row.userId), row.level, songid, artistid, row.sessionId, row.location, row.userAgent)\n",
    "        cur.execute(insert_songplays, songplay_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicando a função process_log_data\n",
    "\n",
    "A função `process_log_data` recebe dois parametros, o cursor e o caminho do arquivo json.\n",
    "\n",
    "Logo no início, lemos esse json com a função do pandas `read_json` e alocamos esse dataframe na variável `df`.\n",
    "\n",
    "Como o time de análise quer os dados apenas da página `NextSong`, filtramos nosso dataframe para conter apenas os registros dessa página.\n",
    "\n",
    "#### Inserindo os dados na tabela dimensão time\n",
    "\n",
    "Para nossa tabela dimensão `time` só precisaremos de uma coluna do dataframe inicial, selecionamos essa coluna com o comando `df['ts']` mas precisamos converter cada registro desse para datetime, por isso, chamamos a função do pandas `to_datetime` e definimos a unidade para milisegundos, atribuindo à variável `t`.\n",
    "\n",
    "Em seguida, iniciamos nossa lista que receberá os dados para cada campo da query para inserir os dados na tabela.\n",
    "Usamos um for loop para percorrer todos os registros da variável `t` e dentro desse loop, separamos as informações que nossa query `insert_time` precisa e adicionamos à nossa lista `time_data`.\n",
    "Com essa lista, faremos um dataframe, e para isso atribuimos os nomes das colunas em `column_labels` e logo depois criamos o dataframe `time_df`.\n",
    "\n",
    "Agora só precisamos percorrer todas as linhas desse dataframe com um for loop e executar a query `insert_time`, lembrando que precisamos usar afunção nativa `list()` para que cada linha do dataframe seja um array/list.\n",
    "Os dados da tabela dimensão tempo estão inseridos!\n",
    "\n",
    "\n",
    "#### Inserindo os dados na tabela dimensão users\n",
    "\n",
    "Esse é simples, vamos selecionar as colunas do dataframe inicial `df` e atribuimos à variável `user_df`.\n",
    "\n",
    "Então usamos um for loop para percorrer cada linha e executar a query `insert_user`, passando cada linha do dataframe como um array/list.\n",
    "\n",
    "\n",
    "### Inserindo os dados na tabela fato song_plays\n",
    "\n",
    "##### Primeiro precisamos encontrar o id da música e do artista\n",
    "\n",
    "No primeiro momento, vamos usar um for loop para percorrer cada linha do dataframe inicial.\n",
    "Em cada etapa desse for, vamos executar aquela query `song_select` passando o nome da música, o nome do artista e o tempo de duração para dentro da query, assim poderemos selecionar as mśsicas que tenha o nome da música, o nome do artista e o tempo de duração igual, para termos acesso ao id da música e ao id do artista, pois essas informações estão nas tabela dimensão `artists` e `songs`, e não nosso dataframe inicial, que não dispõe desses dados.\n",
    "\n",
    "Atribuimos cada resultado à variável results, em seguida usamos um `if`, se results for `True`, separamos os resultados do id da música, id do artista e duração da música. Se for `False` essas variáveis receberão `None`.\n",
    "\n",
    "##### Continuando para inserir os dados na tabela fato song_plays\n",
    "\n",
    "Iniciamos uma variável `songplay_data` para receber os dados necessários para inserir os dados na tabela `song_plays`. Essa variável receberá, na ordem exata, os dados de cada linha do dataframe inicial para passar os valores para dentro da query `insert_songplays`, entre essas variáveis, as duas `songid` e `artistid` que selecionameos agora pouco.\n",
    "\n",
    "Em seguida, executamos o cursor para inserir esses dados separados na query `insert_songplays` e nossa tabela fato está alimentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamamos a função `main_etl` para iniciar nossa lógica criada ate aqui e executar os comandos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
