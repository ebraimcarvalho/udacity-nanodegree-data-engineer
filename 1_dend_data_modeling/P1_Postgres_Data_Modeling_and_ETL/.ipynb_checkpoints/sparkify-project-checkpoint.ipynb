{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkify Project\n",
    "\n",
    "Este projeto foi proposto pelo Nanodegree Data Engineer da Udacity.\n",
    "\n",
    "### Introdução\n",
    "\n",
    "**Modelagem de Dados com Postgres e ETL pipeline**\n",
    "\n",
    "Uma startup chamada Sparkify quer analisar os dados que eles tem coletados das músicas e atividades dos usuários no seu novo aplicativo de streaming de música. O time de análises está interessando em entender quais músicas os usuários estão ouvindo. Atualmente, eles não tem uma forma fácil de consultar esses dados, que estão num diretório em formato JSON, um com os dados dos logs das atividades dos usuarios no aplicativo e outro com os metadados das musicas na biblioteca do aplicativo.\n",
    "\n",
    "\n",
    "Eles gostariam que um engenheiro de dados criasse um banco de dados Postgres com as tabelas desenvolvidas para otimizar as consultas das análises das músicas reproduzidas. Seu papel é criar um database schema e um ETL pipeline para estas análises.\n",
    "\n",
    "### Descrição do Projeto\n",
    "\n",
    "Neste projeto, iremos aplicar o que aprendemos em modelagem de dados com Postgres e construir um pipeline para extrair, transformar e carregar os dados no banco de dados usando Python. Para completar o projeto, precisaremos definir as tabelas fatos e dimensão, usando um star schema para facilitar as consultas do time de análise de dados, além de escrever um pipeline de dados para transferir os dados dos arquivos locais em formato JSON para estas tabelas criadas no Postgres, usando Python e SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste em extrair os dados de duas fontes:\n",
    "\n",
    "- song_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"num_songs\": 1, \n",
    "\"artist_id\": \"ARMJAGH1187FB546F3\", \n",
    "\"artist_latitude\": 35.14968, \n",
    "\"artist_longitude\": -90.04892, \n",
    "\"artist_location\": \"Memphis, TN\", \n",
    "\"artist_name\": \"The Box Tops\", \n",
    "\"song_id\": \"SOCIWDW12A8C13D406\", \n",
    "\"title\": \"Soul Deep\", \n",
    "\"duration\": 148.03546, \n",
    "\"year\": 1969\n",
    "}`\n",
    "\n",
    "- log_data: Exemplo de cada registro\n",
    "\n",
    "`{\n",
    "\"artist\":\"Des'ree\",\n",
    "\"auth\":\"Logged In\",\n",
    "\"firstName\":\"Kaylee\",\n",
    "\"gender\":\"F\",\n",
    "\"itemInSession\":1,\n",
    "\"lastName\":\"Summers\",\n",
    "\"length\":246.30812,\n",
    "\"level\":\"free\",\n",
    "\"location\":\"Phoenix-Mesa-Scottsdale, AZ\",\n",
    "\"method\":\"PUT\",\n",
    "\"page\":\"NextSong\",\n",
    "\"registration\":1540344794796.0,\n",
    "\"sessionId\":139,\n",
    "\"song\":\"You Gotta Be\",\n",
    "\"status\":200,\n",
    "\"ts\":1541106106796,\n",
    "\"userAgent\":\"\\\"Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/537.36 (KHTML, like Gecko) Chrome\\/35.0.1916.153 Safari\\/537.36\\\"\",\n",
    "\"userId\":\"8\"\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print da tabela dos logs de eventos dos usuários:\n",
    "\n",
    "![Screenshot of log_data](images/log-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo Schema e tabelas de relacionamentos\n",
    "\n",
    "Como o intuito é facilitar as consultas para a área de análise de dados executar e obter dados para futuras análises, vamos modelar seguindo o modelo Star Schema, um processamento OLAP (On-line Analytical Processing), que é desenhado para performar melhor e tem as seguintes características:\n",
    "\n",
    "- Aplicação: No nível estratégico, auxilia na análise empresarial e tomada de decisões;\n",
    "- Funcionalidade: Gera análises e relatórios gerenciais com leitura otimizada;\n",
    "- Estrutura de dados: Poucos detalhes, pois tem alto nível de sumarização;\n",
    "- Armazenamento dos dados: Utiliza-se da Data Warehouse para otimizar o desempenho da grande quantidade de dados;\n",
    "- Usuários: Destinados aos gestores e time analítico;\n",
    "- Frequência de utilização: Baixa, conforme programação da empresa;\n",
    "- Volatilidade: Dados não sofrem alterações, pois os usuários apenas realizarão sua leitura.\n",
    "\n",
    "O OLAP (On-line Analytical Processing) é voltado para a tomada de decisões, proporciona uma visão dos dados orientado à análise, além de uma navegação rápida e flexível. O OLAP recebe dados do OLTP (On-line Transactional Processing) para que possa realizar as análises. Essa carga de dados acontece conforme a necessidade da empresa. Sendo um sistema para tomada de decisões, não realiza transações (INSERT, UPDATE, DELETE) pois sua finalidade são consultas. Possui dados atuais e históricos e não há necessidade de backups regularmente, sendo que ele possui informações do OLTP. Caso algo aconteça com a base OLAP basta fazer uma carga novamente.\n",
    "\n",
    "\n",
    "### Tabelas\n",
    "\n",
    "Vamos agora começar a definir nossas tabelas fato e tabelas dimensão, lembrando que uma tabela fato é um evento, uma venda, uma transação ocorrida, um fato transacional que ocorreu no nosso sistema, no caso, quando um usuário acessa o aplicativo sparkify e clica numa música para ouvir, esses dados da musica selecionada, do usuário, localização, página em que ele estava e etc, fazem parte do evento \"tocar música\", esse é um bom candidato para nossa tabela fato. As tabelas fatos geralmente são de dados numéricos e não categóricos. Resumindo:\n",
    "\n",
    "A tabela fato é a principal tabela do Data Warehouse, ela vai conectar nas dimensões. Nessa tabela são armazenadas duas coisas: as métricas, que são os fatos propriamente ditos, e as foreign keys, chaves que servem para ligar os dados das dimensões com a fato. Ou seja, a tabela fato é composta pelas métricas, que são tudo aquilo que a empresa quer medir, junto com as foreign keys, chaves que ligam às dimensões que descrevem essas métricas. As métricas são utilizadas para medir, quantificar algo, são sempre números provenientes de transações da empresa. Tudo que a empresa quer mensurar é métrica, geralmente sendo o que o usuário quer medir.\n",
    "\n",
    "Mas o que é uma Foreign Key? É uma chave estrangeira que serve para relacionar os dados entre as tabelas fato e dimensão.\n",
    "\n",
    "Já as tabelas Dimensão, são as categorias de cada entidade envolvida na tabela fato, que podem ser usadas para trazer mais informações sobre os dados e facilitar as análises. Os dados do usuário que reproduziu uma música é um exemplo para uma tabela dimensão, o nome, sobrenome, gênero, nível na aplicação são os dados dessa dimensão. Os dados da música selecionadas já fazem parte de outra dimensão, como o nome da música, o ano de lançamento, tempo de duração e etc.\n",
    "\n",
    "A Dimensão possui característica descritiva dentro do DW. Ela qualifica as informações provenientes da tabela Fato. Através dela é possível analisar os dados sob múltiplas perspectivas.\n",
    "\n",
    "### Fact Table\n",
    "\n",
    "- songplays: registros nos dados log de eventos associados com músicas reproduzidas\n",
    "    - songplay_id, \n",
    "    - start_time, \n",
    "    - user_id, \n",
    "    - level, \n",
    "    - song_id, \n",
    "    - artist_id, \n",
    "    - session_id, \n",
    "    - location, \n",
    "    - user_agent\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "- users: usuários do aplicativo\n",
    "    - user_id, \n",
    "    - first_name, \n",
    "    - last_name, \n",
    "    - gender, \n",
    "    - level\n",
    "    \n",
    "    \n",
    "- songs: músicas no banco de dados\n",
    "    - song_id, \n",
    "    - title, \n",
    "    - artist_id, \n",
    "    - year, \n",
    "    - duration\n",
    "    \n",
    "    \n",
    "- artists: Artistas das músicas no banco de dados\n",
    "    - artist_id, \n",
    "    - name, \n",
    "    - location, \n",
    "    - latitude, \n",
    "    - longitude\n",
    "\n",
    "\n",
    "- time: Timestamps dos registros das músicas reproduzidas quebrados em unidades específicas para facilitar futuras análises\n",
    "    - start_time, \n",
    "    - hour, \n",
    "    - day, \n",
    "    - week, \n",
    "    - month, \n",
    "    - year, \n",
    "    - weekday"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
